<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tensor是如何让你的内存/显存泄漏的 | Fatescript</title> <meta name="author" content="Feng Wang"> <meta name="description" content="Fatescript's website "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Fatescript"> <meta property="og:type" content="article"> <meta property="og:title" content="Fatescript | Tensor是如何让你的内存/显存泄漏的"> <meta property="og:url" content="https://fatescript.github.io/blog/2022/tensor-memory-leak/"> <meta property="og:description" content="Fatescript's website "> <meta property="og:image" content="https://fatescript.github.io/blog_icon.png"> <meta property="og:image:secure_url" content="https://fatescript.github.io/blog_icon.png"> <meta property="og:image:width" content="300"> <meta property="og:image:height" content="300"> <meta property="og:locale" content="cn"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Tensor是如何让你的内存/显存泄漏的"> <meta name="twitter:description" content="Fatescript's website "> <meta name="twitter:image" content="https://fatescript.github.io/blog_icon.png"> <meta name="twitter:site" content="@wangfeng0315"> <meta name="twitter:creator" content="@wangfeng0315"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://fatescript.github.io/blog/2022/tensor-memory-leak/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Fatescript</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">bookshelf</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tensor是如何让你的内存/显存泄漏的</h1> <p class="post-meta">May 20, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/code"> <i class="fas fa-hashtag fa-sm"></i> code</a>     ·   <a href="/blog/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   <a href="/blog/category/engineering"> <i class="fas fa-tag fa-sm"></i> engineering</a>   </p> </header> <article class="post-content"> <div> <div class="toc"> <details> <summary class="toc-summary">Table of Contents</summary> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h4"><a href="#%E5%89%8D%E8%A8%80">前言</a></li> <li class="toc-entry toc-h4"><a href="#%E8%B5%B7%E6%BA%90">起源</a></li> <li class="toc-entry toc-h4"><a href="#%E5%A4%8D%E7%8E%B0">复现</a></li> <li class="toc-entry toc-h4"><a href="#%E6%8E%A2%E7%B4%A2">探索</a></li> <li class="toc-entry toc-h4"><a href="#%E8%A7%A3%E6%83%91">解惑</a></li> <li class="toc-entry toc-h4"><a href="#%E5%90%8E%E8%AE%B0">后记</a></li> </ul> </details> </div> <div id="markdown-content"> <h4 id="前言">前言</h4> <p>本文适合算法研究员/工程师阅读，如果你遇到奇怪的内存泄漏问题，说不定本文能帮你找到答案，解答疑惑。 虽然在大部分场景下，程序的内存泄漏都和数据息息相关。但是读完本文你就会了解，没有被正确使用的Tensor也会导致内存和显存的泄漏。</p> <h4 id="起源">起源</h4> <p>某次组会的时候，同事报告了一个很好玩的issue：我司某组的一个codebase出现了奇怪的泄漏现象，奇怪的点有以下几个方面：<br> （1）不同的模型，内存/显存泄漏的现象不一样。比如A模型和B模型泄露的速度是不一样的<br> （2）训练同一个模型的时候，如果在dataset中增加了数据量，相比不加数据，会在更早的epoch就把内存泄漏完。<br> 是不是听起来现象非常离谱，本着”code never lies“的世界观，我开始探求这个现象的真正原因。</p> <h4 id="复现">复现</h4> <p>要想解决一个大的问题，首先就要降低问题的复杂度。最小复现代码是我们找问题的基础，而这个写最小复现代码的过程其实也是遵循了一定套路的，此处一并分享给大家：</p> <ul> <li>如果突然出现了历史上没有出现过的问题（比如在某个版本之后突然内存开始泄漏了），用git bisect找到 first bad commit（前提项目管理的比较科学，不会出现很多feature杂糅在一个commit里面；还有就是git checkout之后复现问题的成本不高）。如果bisect大法失效，考虑下面的复现流程。</li> <li>首先排除data的问题，也就是只创建一个dataloader，让这个loader不停地供数据，看看内存会不会涨（通常data是一系列对不上点、内存泄漏的重灾区）。</li> <li>其次排除训练的问题，找一个固定数据，不停地让网络训练固定数据进行训练/推理，看看是否发生泄漏。这一步主要是检查模型、优化器等组件的问题（通常模型本身不会发生泄漏，这一步经常能查出来一些自定义op的case）</li> <li>最后就是检查一些外围组件了。比如各种自己写的utils/misc的内容。这块通常不是啥重灾区。</li> </ul> <p>最后给出来我的最小复现（loguru可以换成print）：</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td> <td class="code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">loguru</span> <span class="kn">import</span> <span class="n">logger</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">psutil</span>


<span class="k">def</span> <span class="nf">log_device_usage</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">):</span>
    <span class="n">mem_Mb</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nc">Process</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">getpid</span><span class="p">()).</span><span class="nf">memory_info</span><span class="p">().</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">cuda_mem_Mb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memory_allocated</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">iter </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s">, mem: </span><span class="si">{</span><span class="nf">int</span><span class="p">(</span><span class="n">mem_Mb</span><span class="p">)</span><span class="si">}</span><span class="s">Mb, gpu mem:</span><span class="si">{</span><span class="nf">int</span><span class="p">(</span><span class="n">cuda_mem_Mb</span><span class="p">)</span><span class="si">}</span><span class="s">Mb</span><span class="sh">"</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">leak</span><span class="p">():</span>
    <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">log_iter</span> <span class="o">=</span> <span class="mi">20000</span>
    <span class="nf">log_device_usage</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">val</span> <span class="o">+=</span> <span class="n">value</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">()</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">%</span> <span class="n">log_iter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">log_device_usage</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">leak</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>试着运行一下，你就会发现你的内存和显存开始起飞了（内存泄漏的比显存更快一些），泄漏到一定程度，整个程序就会卡死，过一段时间就会被kill掉。作为对比，将<code class="language-plaintext highlighter-rouge">requires_grad_()</code>删掉（或者在后面加上<code class="language-plaintext highlighter-rouge">detach()</code>），你就可以看到没有泄漏发生的log了。</p> <p>写完了复现之后，同事问了我俩问题，大家也可以提前思考一下：</p> <ol> <li>为啥这个程序会出现内存/显存泄漏？</li> <li>为啥明明在gpu上的tensor会泄漏内存？</li> </ol> <h4 id="探索">探索</h4> <p>首先第二个问题很好理解，<strong>因为虽然在概念上，torch中的tensor是在gpu上的，但是也只是数据的storage在gpu上，除了在显存上存储的数据，tensor的一些其他信息（比如shape，stride和output_nr等）肯定也是要占据一定内存的。所以在cuda available的时候，内存和显存都会泄漏。</strong></p> <p>那么第一个问题是因为啥呢？我一时间也难以想明白，于是我打算直接通过torch的源码去找问题的答案。这个过程略长一些，想要看结论的读者可以直接跳到解惑部分。如果对torch内部的东西稍微感兴趣，可以继续看下去。 因为torch里面有很多code是生成出来的（有机会我们可以讲一讲torch的code gen），所以我们需要先编译一下torch（我用的commit hash是2367face）。因为写torch的cuda extension的时候，要使用Tensor就会需要 include &lt;ATen/ATen.h&gt;，以此为线索我最后定位到了一个叫做TensorBody.h的文件，通过fzf在torch/include/ATen/core下的TensorBody.h文件中找到了inplace add的定义，源码如下（torch中inplace都是在原来的名字后面加_，比如add和add_）。</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kr">inline</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">add_</span><span class="p">(</span><span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">other</span><span class="p">,</span> <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Scalar</span> <span class="o">&amp;</span> <span class="n">alpha</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">add__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span> <span class="n">other</span><span class="p">,</span> <span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure> <p>再通过<a href="https://github.com/ggreer/the_silver_searcher" rel="external nofollow noopener" target="_blank">ag</a>找<code class="language-plaintext highlighter-rouge">add__Tensor</code>的定义，最后在torch/csrc/autograd/generated文件夹下面的VariableTypeEverything.cpp文件找到定义。这个文件其实是多个VariableType_{0,1,2,3}.cpp开头的文件拼接成的。在VariableType_3.cpp中我们可以找到<code class="language-plaintext highlighter-rouge">add__Tensor</code>的定义。此处我们精简一下和我们的case相关的部分方便理解。</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td> <td class="code"><pre><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">add__Tensor</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKeySet</span> <span class="n">ks</span><span class="p">,</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">&amp;</span> <span class="n">other</span><span class="p">,</span> <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Scalar</span> <span class="o">&amp;</span> <span class="n">alpha</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">self_</span> <span class="o">=</span> <span class="n">unpack</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="s">"self"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">other_</span> <span class="o">=</span> <span class="n">unpack</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="s">"other"</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">_any_requires_grad</span> <span class="o">=</span> <span class="n">compute_requires_grad</span><span class="p">(</span> <span class="n">self</span><span class="p">,</span> <span class="n">other</span> <span class="p">);</span>
 
    <span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">_any_requires_grad</span><span class="p">;</span>
    <span class="n">check_inplace</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">_any_requires_grad</span><span class="p">);</span>
    <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">original_self</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span> <span class="n">grad_fn</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">_any_requires_grad</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">(</span><span class="k">new</span> <span class="n">AddBackward0</span><span class="p">(),</span> <span class="n">deleteNode</span><span class="p">);</span>
      <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">collect_next_edges</span><span class="p">(</span> <span class="n">self</span><span class="p">,</span> <span class="n">other</span> <span class="p">));</span>
      <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">other_scalar_type</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">();</span>
      <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">;</span>
      <span class="n">grad_fn</span><span class="o">-&gt;</span><span class="n">self_scalar_type</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="p">{</span>
      <span class="n">at</span><span class="o">::</span><span class="n">AutoDispatchBelowAutograd</span> <span class="n">guard</span><span class="p">;</span>
      <span class="n">at</span><span class="o">::</span><span class="n">redispatch</span><span class="o">::</span><span class="n">add_</span><span class="p">(</span><span class="n">ks</span> <span class="o">&amp;</span> <span class="n">c10</span><span class="o">::</span><span class="n">after_autograd_keyset</span><span class="p">,</span> <span class="n">self_</span><span class="p">,</span> <span class="n">other_</span><span class="p">,</span> <span class="n">alpha</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">rebase_history</span><span class="p">(</span><span class="n">flatten_tensor_args</span><span class="p">(</span> <span class="n">self</span> <span class="p">),</span> <span class="n">grad_fn</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>这里我们顺便来看一下<code class="language-plaintext highlighter-rouge">add__Tensor</code>函数在干啥，<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/VariableTypeManual.cpp#L43-L64" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">unpack</code></a>方法其实就是对tensor的一个检查，unpack后的code简单来说就是计算一下input tensor是否需要梯度（这个会影响到前向过程对于输出tensor的grad_fn的设置），如果需要梯度，就会进行图的构建（也就是设置tensor对应的一些属性），之后用dispatcher发送add的kernel，完成tensor的加法运算。torch中其他的op如sub，sigmoid等都是遵循一样的逻辑（因为torch里面前向过程创建图的逻辑是完全一样的，和具体的op类型无关，所以这些op才可以通过代码生成出来）。</p> <p>解释完了函数的逻辑，我们来重新看一下泄漏的问题。</p> <p>如果我们注释掉<code class="language-plaintext highlighter-rouge">grad_fn-&gt;set_next_edges(collect_next_edges( self, other ));</code> 或 <code class="language-plaintext highlighter-rouge">rebase_history(flatten_tensor_args( self ), grad_fn);</code> 这两行code中的任意一行，那么都不会出现内存/显存泄漏的现象，由此我们有理由怀疑是在构建动态图的过程中产生了内存泄漏的。</p> <p>又因为<code class="language-plaintext highlighter-rouge">rebase_history</code>是后面才被调用的，所以<code class="language-plaintext highlighter-rouge">set_next_edges</code>过程肯定只是出现泄漏的一个诱因，真正发生泄漏的位置肯定在后调用的位置，由此我们进一步来看<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/VariableTypeUtils.h#L90-L110" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">rebase_history</code></a>的实际代码<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/variable.cpp#L142-L166" rel="external nofollow noopener" target="_blank">实现</a>。从源码逻辑来看，大部分是检查和确保一些属性的逻辑，核心在于<code class="language-plaintext highlighter-rouge">set_gradient_edge(self, std::move(gradient_edge));</code>这一句。由此，我们来看<code class="language-plaintext highlighter-rouge">set_gradient_edges</code>的逻辑，当然，为方便理解，下面的code做了一些精简（全部code的参考链接： <a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/variable.cpp#L234-L247" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">set_gradient_edge</code></a>，<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/variable.cpp#L133-L140" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">materialize_autograd_meta</code></a>，<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/variable.cpp#L311-L315" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">get_auto_grad_meta</code></a>）</p> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="kt">void</span> <span class="nf">set_gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="n">Edge</span> <span class="n">edge</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">materialize_autograd_meta</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
  <span class="n">meta</span><span class="o">-&gt;</span><span class="n">grad_fn_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">edge</span><span class="p">.</span><span class="n">function</span><span class="p">);</span>
  <span class="n">meta</span><span class="o">-&gt;</span><span class="n">output_nr_</span> <span class="o">=</span> <span class="n">edge</span><span class="p">.</span><span class="n">input_nr</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">AutogradMeta</span><span class="o">*</span> <span class="nf">materialize_autograd_meta</span><span class="p">(</span><span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">p</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">unsafeGetTensorImpl</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">autograd_meta</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">p</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">());</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nf">get_autograd_meta</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">AutogradMeta</span><span class="o">*</span> <span class="nf">get_autograd_meta</span><span class="p">(</span><span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">unsafeGetTensorImpl</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">autograd_meta</span><span class="p">());</span>
<span class="p">}</span></code></pre></figure> <p>看到这里，基本上熟悉pytorch中对于图定义的同学大概就能知道是什么原因了。关于pytorch中forward过程构建图的原理，可以参考官网的<a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/" rel="external nofollow noopener" target="_blank">blog</a>，作为一个基础概念，我们只需要了解：<strong>动态图就是在forward过程中进行图的“创建”，在backward过程完成图的“销毁”。</strong></p> <p>现在让我们回到数据结构中Graph（图）的概念。在一个自动求导系统中，我们可以将Graph中的<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/edge.h#L14" rel="external nofollow noopener" target="_blank">Edge</a>（边）简单地理解为一个tensor，Graph中<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/function.h#L99" rel="external nofollow noopener" target="_blank">Node</a>（节点）的概念理解为算子。比如在torch里写 <code class="language-plaintext highlighter-rouge">c = a + b</code>，其实就是表示有一个a 表示的Edge和一个b代表的Edge连接到一个add的Node（节点）上，这个Node又会连接到一个叫做c的Edge上（下面是一个用<a href="https://github.com/mermaid-js/mermaid" rel="external nofollow noopener" target="_blank">mermaid</a>画的一个示意图，其中Edge用矩形表示，Node用圆表示。不难看出，add就是一个入度为2，出度为1的Node）。</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/blog/tensor_graph-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/blog/tensor_graph-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/blog/tensor_graph-1400.webp"></source> <img src="/assets/blog/tensor_graph.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">c = a + b的图表示</figcaption> </figure> ​ </div> <p>既然我们有了图，那么就需要有一些结构保存一部分基本的图信息，这些基本图信息会在自动求导（autograd）的时候使用。在torch中，AutogradMeta就是包含了诸如tensor的autograd历史、hooks等信息的结构，而导致我们内存/显存泄漏的罪魁祸首也正是这个<a href="https://github.com/pytorch/pytorch/blob/v1.10.1/torch/csrc/autograd/variable.h#L190" rel="external nofollow noopener" target="_blank">AutogradMeta</a>。 现在，我们已经知道memory实际上泄漏的是啥了。跳回我们写的code，结合gc机制，想一想问题1你是否知道了答案。</p> <h4 id="解惑">解惑</h4> <p>至此，我们基本上就可以把问题1解释清楚了：<strong>在Tensor的requires_grad为True的时候，Tensor的每次运算都会导致需要保存一份AutogradMeta信息，对应的Tensor也会被加入到计算图中。即使表面上来看你只是做了一些inplace add的操作，但是其实在torch内部，那个临时的Tensor已经进入到了图里，成为了图的一个Edge，且引用计数 + 1，自然是要占据空间的。如果你的Tensor不requires_grad，那么就是只是进行运算，不会有Meta等信息存在，那个暂时生成的Tensor就会引用计数清0被gc了，自然也不会有内存泄漏了。</strong> 除了问题1之外，结合上面介绍的内容，我们也能理解，下面一段非常pythonic的code在pytorch里面并不科学的原因。</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>
<span class="n">total_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span></code></pre></figure> <p>现在，让我们从最小复现代码回归到codebase，其实我给出的复现里面的代码中的value就是loss，很多时候炼丹师会想要看一下loss的均值/最大值等统计信息，经常会用一个meter保存历史信息，也就对应了复现代码里面的val。 很多奇怪的现象到此也就说的通了，比如不同模型泄漏速度不一样，就是因为不同的模型loss的数量是不一样的，泄漏的速度自然也是不一样的；再比如增加数据会使得同一个模型在更早的epoch到达OOM状态，是因为当数据增加的时候一个epoch内的iter数就会变多，自然会有在更早的epoch把内存泄漏完的现象；曾经能训练的模型加了数据之后也有可能因此变得无法训练。</p> <h4 id="后记">后记</h4> <p>也许下面这句话对炼丹师来说听起来有些反直觉，但我觉得还是有必要声明一下：<strong>无论python前端中tensor看起来是如何动态地进行运算，概念上计算图中的每个节点都无法被inplace修改。</strong></p> <p>在理解了本文要介绍的原理后，我们也可以轻易写一些reviewer看起来好像没啥问题的泄漏程序了（逃</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">leak</span><span class="p">():</span>
    <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">()</span>  <span class="c1"># 比如这个requires_grad_是在某个地方偷偷加的
</span>    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">log_iter</span> <span class="o">=</span> <span class="mi">20000</span>
    <span class="nf">log_device_usage</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># 这个1在torch里面会表示为一个cpu tensor
</span>        <span class="k">if</span> <span class="n">count</span> <span class="o">%</span> <span class="n">log_iter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">log_device_usage</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure> <p>为了更好的表示上述代码在执行过程中发生了什么，我用<a href="https://github.com/3b1b/manim" rel="external nofollow noopener" target="_blank">manim</a>写了一个动画来提供更直观的解释，放在结尾也是希望读者能在读完文章后，稍微让头脑休息一下吧：）</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/blog/manim.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/blog/manim.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/blog/manim.gif-1400.webp"></source> <img src="/assets/blog/manim.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>​</p> </div> </div> </article> <div class="pagenav"> <a class="previous-post" href="/blog/2022/copybara/"> <span>⬅️</span><br> <span class="smaller-title"><strong>copybara：关于我只是想在仓库间做代码搬运这件事</strong></span> </a> <a class="next-post" href="/blog/2021/details-of-deep-learning-engineering/"> <span>➡️</span><br> <span><strong>关于炼丹，你是否知道这些细节？</strong></span> </a> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"FateScript/FateScript.github.io","data-repo-id":"R_kgDOH71xZQ","data-category":"Announcements","data-category-id":"DIC_kwDOH71xZc4CZgZI","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"zh-CN",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Feng Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-EH97TKHJR3"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EH97TKHJR3");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>