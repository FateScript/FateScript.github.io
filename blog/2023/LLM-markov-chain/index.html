<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>LLMs as Markov Chain | Fatescript</title> <meta name="author" content="Feng Wang"> <meta name="description" content="看待LLM的新视角"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Fatescript"> <meta property="og:type" content="article"> <meta property="og:title" content="Fatescript | LLMs as Markov Chain"> <meta property="og:url" content="https://fatescript.github.io/blog/2023/LLM-markov-chain/"> <meta property="og:description" content="看待LLM的新视角"> <meta property="og:image" content="https://fatescript.github.io/blog_icon.png"> <meta property="og:image:secure_url" content="https://fatescript.github.io/blog_icon.png"> <meta property="og:image:width" content="300"> <meta property="og:image:height" content="300"> <meta property="og:locale" content="cn"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="LLMs as Markov Chain"> <meta name="twitter:description" content="看待LLM的新视角"> <meta name="twitter:image" content="https://fatescript.github.io/blog_icon.png"> <meta name="twitter:site" content="@wangfeng0315"> <meta name="twitter:creator" content="@wangfeng0315"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://fatescript.github.io/blog/2023/LLM-markov-chain/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Fatescript</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">bookshelf</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLMs as Markov Chain</h1> <p class="post-meta">June 3, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/math"> <i class="fas fa-hashtag fa-sm"></i> math</a>   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> LLM</a>     ·   <a href="/blog/category/engineering"> <i class="fas fa-tag fa-sm"></i> engineering</a>   </p> </header> <article class="post-content"> <div> <div class="toc"> <details> <summary class="toc-summary">Table of Contents</summary> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h4"><a href="#%E5%89%8D%E8%A8%80">前言</a></li> <li class="toc-entry toc-h4"> <a href="#karpathy%E7%9A%84%E8%A7%82%E7%82%B9">Karpathy的观点</a> <ul> <li class="toc-entry toc-h5"><a href="#context-length%E4%B8%8Etokenizer">context length与tokenizer</a></li> <li class="toc-entry toc-h5"><a href="#vocab_size%E4%B8%8Econtext-length%E5%86%B3%E5%AE%9A%E4%BA%86%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%E7%9A%84%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4">vocab_size与context length决定了马尔可夫链的状态空间</a></li> <li class="toc-entry toc-h5"><a href="#%E4%BB%8Emarkov-chain%E8%A7%86%E8%A7%92%E7%9C%8B%E8%AE%AD%E7%BB%83">从Markov Chain视角看训练</a></li> </ul> </li> <li class="toc-entry toc-h4"> <a href="#lm-as-markov-chain%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%A7%E8%B4%A8">LM as Markov chain的一些性质</a> <ul> <li class="toc-entry toc-h5"><a href="#%E6%80%A7%E8%B4%A8%E4%B8%8E%E5%90%AF%E5%8F%91">性质与启发</a></li> <li class="toc-entry toc-h5"><a href="#%E5%B1%95%E5%BC%80%E7%9C%8Blm%E7%9A%84%E7%89%B9%E6%80%A7">展开看LM的特性</a></li> </ul> </li> <li class="toc-entry toc-h4"> <a href="#%E6%96%B0%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84old-things">新视角下的Old things</a> <ul> <li class="toc-entry toc-h5"><a href="#prompt-engineering">Prompt Engineering</a></li> <li class="toc-entry toc-h5"><a href="#in-context-learning">In-Context Learning</a></li> <li class="toc-entry toc-h5"><a href="#cot">CoT</a></li> </ul> </li> <li class="toc-entry toc-h4"><a href="#random-but-not-random">Random, but not random</a></li> <li class="toc-entry toc-h4"><a href="#citation">Citation</a></li> <li class="toc-entry toc-h4"><a href="#reference">Reference</a></li> </ul> </details> </div> <div id="markdown-content"> <h4 id="前言">前言</h4> <p>几个月之前，<a href="https://karpathy.ai/" rel="external nofollow noopener" target="_blank">Andrej Karpathy</a> 发布了一个<a href="https://twitter.com/karpathy/status/1645115622517542913" rel="external nofollow noopener" target="_blank">推特</a><sup>[1]</sup>，给出了一个看待语言模型(language model，下称LM)行为的新视角：LM可以看作有限状态的马尔可夫链（finite-state Markov Chain）。在最近一段时间和LLM（large language model）的交互过程中，以这个马尔可夫链的视角作为基础，笔者对于LLM的一些行为有了进一步的理解与认知。写这篇文章，一方面是为了分享Karpathy的观点，另一方面则是帮助大家从实践的视角进一步理解/预测语言模型的一些行为。</p> <p>本文会在第一个部分介绍LM为什么可以被看作是一个Markov chain；之后会从以这个视角进一步展开，聊一聊Markov chain视角下的Prompt Engineer、In-Context Learning以及一些LM展现出来的有趣特性。</p> <h4 id="karpathy的观点">Karpathy的观点</h4> <p>为了照顾一些初学者，这个部分会介绍地尽量详细一些，已经了解为什么LM可以被看作是Markov chain的读者可以跳过这个部分。</p> <p>想要体验最原汁原味的介绍可以移步Karpathy写的<a href="https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing" rel="external nofollow noopener" target="_blank">colab</a><sup>[2]</sup>。</p> <h5 id="context-length与tokenizer">context length与tokenizer</h5> <p>LM从本质上来看，就是接受一堆文字作为输出，然后不断预测下一个文字的模型。为了通俗一些，我们举一个例子，假设我们有一个窗口大小为4的LM，这个LM接受的输入给LM这样一段话“今天天气”，LM就会预测下一个字是“真”，接着我们把“真”放入“今天天气”后面，同时保持窗口大小不变，LM接受到的输入就是“天天气真”，LM就会预测下一个字是“好”，接着把好放在之前的句子后面，以此类推，最后我们就可以得到“今天天气真好。”的输出。</p> <p>因为LM的输入需要是固定的长度，为了统一，我们就会称这个固定长度为<strong>context length</strong>。上文的例子中的LM的context length的就是4，这也就意味着这个LM一次性接受4个词的输入，并且预测下一个词是什么。</p> <p>但是，LM是无法接受文字作为输入的，对于所有的LM来说，都需要<strong>tokenizer</strong>将文字输入转换成token。以<a href="https://arxiv.org/pdf/2302.13971.pdf" rel="external nofollow noopener" target="_blank">LLaMA</a>的<a href="https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/tokenizer.py#L13" rel="external nofollow noopener" target="_blank">tokenizer</a>为例子，在不考虑bos(begin of sentence，即&lt;s&gt; )和eos(end of sentence，即&lt;/s&gt;)符号的情况下，句子”Hello world”会被转换成 \([15043, 3186]\) 的输入，之后这个输入就可以被LM接收，从而预测下一个单词。</p> <p>下面的code给出了一个具体的示例来方便理解：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello world</span><span class="sh">"</span><span class="p">,</span> <span class="n">bos</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">eos</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&lt;&lt;&lt;</span> <span class="p">[</span><span class="mi">15043</span><span class="p">,</span> <span class="mi">3186</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">([</span><span class="mi">15043</span><span class="p">,</span> <span class="mi">3186</span><span class="p">])</span>
<span class="o">&lt;&lt;&lt;</span> <span class="sh">'</span><span class="s">Hello world</span><span class="sh">'</span>
</code></pre></div></div> <p>为了说明简单，我们后文中的token都采用数字来表示，这样我们就可以把LM的输入看作是一个数字序列，而LM的输出则是一个关于全部token的分布。而tokenizer能够处理的字符集的大小，我们称之为<strong>vocab_size</strong>。</p> <p>假设token只有0和1两种（vocab_size为2），context_length 为2，\(\rightarrow\) 表示数据流向，LM推理[1, 0]输入的过程可以表示为：</p> \[[1, 0] \rightarrow LM \rightarrow [P(0) = 40\%, P(1) = 60\%]\] <p>当然这里预测为0和1的概率是随便给的，只是为了方便理解。</p> <h5 id="vocab_size与context-length决定了马尔可夫链的状态空间">vocab_size与context length决定了马尔可夫链的状态空间</h5> <p>考虑一个最最简单的LM，我们称之为baby-GPT，这个LM的context length为3，token只有[0, 1]两种，那么这个LM的全部状态空间就可以表征为 \([0, 1]\) 的3次笛卡尔积， 也就是说这个baby-GPT的状态空间大小为 \({vocab\_size}^{context\_length} = 2^3 = 8\)。 具体来说，所有的状态空间为 \([0, 0, 0]\), \([0, 0, 1]\), \([0, 1, 0]\), \([0, 1, 1]\), \([1, 0, 0]\), \([1, 0, 1]\), \([1, 1, 0]\), \([1, 1, 1]\)。</p> <p>考虑一个baby-gpt的特定状态，此处我们以 \([0, 0, 1]\) 为例，将这个状态作为baby-gpt的输入， 对应的输出的形式则类似于 \([P(0) = 45\%, P(1) = 55\%]\)，代表下一个token是0或者1的概率。 将这个过程对应到马尔可夫链的角度，我们可以认为 \([0, 0, 1]\) 状态可以转移到 \([0, 1, 0]\) 和 \([0, 1, 1]\) 两个后继状态，转移概率分别为 \(45\%\) 和 \(55\%\) 。</p> <p>下图给出了baby-GPT在初始状态下的每个状态和对应的转移概率。</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/blog/baby_gpt_init-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/blog/baby_gpt_init-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/blog/baby_gpt_init-1400.webp"></source> <img src="/assets/blog/baby_gpt_init.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">baby-gpt初始转移概率</figcaption> </figure> ​ </div> <h5 id="从markov-chain视角看训练">从Markov Chain视角看训练</h5> <p>假设开始训练这个baby-GPT，需要训练的数据序列为”111101111011110”，则baby-GPT实际的训练数据则为：</p> <p>训练数据 01: \([1, 1, 1] \rightarrow 1\)<br> 训练数据 02: \([1, 1, 1] \rightarrow 0\)<br> 训练数据 03: \([1, 1, 0] \rightarrow 1\)<br> 训练数据 04: \([1, 0, 1] \rightarrow 1\)<br> 训练数据 05: \([0, 1, 1] \rightarrow 1\)<br> 训练数据 06: \([1, 1, 1] \rightarrow 1\)<br> 训练数据 07: \([1, 1, 1] \rightarrow 0\)<br> 训练数据 08: \([1, 1, 0] \rightarrow 1\)<br> 训练数据 09: \([1, 0, 1] \rightarrow 1\)<br> 训练数据 10: \([0, 1, 1] \rightarrow 1\)<br> 训练数据 11: \([1, 1, 1] \rightarrow 1\)<br> 训练数据 12: \([1, 1, 1] \rightarrow 0\)</p> <p>在正常训练了模型之后，我们可以得到一个训练好的baby-GPT的权重，此时baby-GPT的状态转移概率相对初始版本已经发生了变化，如下图所示：</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/blog/baby_gpt_trained-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/blog/baby_gpt_trained-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/blog/baby_gpt_trained-1400.webp"></source> <img src="/assets/blog/baby_gpt_trained.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">baby-gpt训练后的转移概率</figcaption> </figure> ​ </div> <p>从上图不难看出，相比初始状态，训练后的baby-GPT在 \([0, 0, 1]\) 状态下更容易生成转移到 \([0, 1, 1]\) (概率从 \(55\%\) 提升到 \(78\%\) )。实际上整个baby-GPT相比初始状态，更容易预测下一个token是1，这也符合训练数据特点：1的数量远远大于0。</p> <p>到这里，基本上大家可以理解为什么LM本质上是一个Markov chain了，也能根据上面提供的例子理解数据是如何影响这个Markov chain的了。</p> <h4 id="lm-as-markov-chain的一些性质">LM as Markov chain的一些性质</h4> <p>本章节讨论LM作为Markov chain会有哪些有趣的性质，以及这些性质对LM的训练和使用有什么启发。</p> <p><strong><span style="color:red">声明：这些性质未必是LLM作为Markov chain一定存在的性质，更多是我个人的看法和符合直觉的思想实验，欢迎提出不一样的看法。 </span></strong></p> <h5 id="性质与启发">性质与启发</h5> <ul> <li>第一个性质肯定是<strong>稀疏性</strong>，这个也很直觉，虽然Markov chain的状态非常多，但是大部分状态之间几乎没有转移概率，因此这个Markov chain是非常稀疏的。<br> 这个性质对LM的训练的启发在于：如果想要让LM在特定场景能够输出一些不常用的字，比如“你”字后面跟个“铋”字，单纯更多地使用“你铋”的话应该是治标不治本的，因为模型是根据context进行转移的，而“你”字只是最后一个token，以“你”字结尾的状态数过于巨大，并不是简单通过加几个训练样本就能解决的。</li> <li>第二个性质在于状态数的<strong>指数爆炸性</strong>，随着LM的vocab_size和context_length增大，Markov Chain中的状态数是几乎以指数倍增长的，在原始训练数据分布不变的情况下，模型建模的难度是减少的。但如果想要更好的效果，模型需要投喂的数据量可能也需要进行某种（感觉是指数的？）形式的scale。至少从直觉上来说，应该存在某种数据规模和context_length之间的scaling law。除此之外，状态的指数爆炸性也在某种程度上能解释为什么LLM会存在涌现能力，很可能状态数达到了某种足够多的状态之后，完成某个任务的知识的建模起来更加容易了。</li> <li>第三个性质是Markov chain中<strong>同构现象普遍存在</strong>。这个同构现象是将Markov chain看作一个图，而这个大图中的部分子图是同构的。比如考虑一个同时具有英文和中文能力的LM，“I want to go home”和“我想回家”在tokenizer看来是完全没有任何关系的两句话（因为tokenizer encode出来的结果完全不一样），但是我们如果站在Markov chain的视角去看这两句话在图里面的结构，很可能是非常相似的。<strong>不同语言的相似语义保证了这种同构现象的存在</strong>。<br> 这个性质对于LM训练的启发在于：如果想要提升LM在某种语言（比如中文）的效果，单纯堆中文语料甚至不一定比中英混合语料更有效。解决A空间中的问题或许可以采用解决空间B中的问题 + 映射回A空间的方式。</li> </ul> <h5 id="展开看lm的特性">展开看LM的特性</h5> <p>首先想聊的是模型的能力(ability)。</p> <p>在最早看到CoT(Chain of Thought)相关的paper<sup>[4]</sup>以及“Let’s think step by step.”<sup>[5]</sup>的魔法提示词(Prompt)之后，我一度不是很理解：通过更改Prompt的方式，模型就比原来更有可能产生期望的输出结果，而且很可能模型在训练阶段都没怎么见到过这个Prompt。 这件事放在计算机视觉领域类比一下，就相当于找到了一个新的图像增强策略，这个增强策略在训练阶段没有使用过，但是却能够在所有的模型上有效提升效果。</p> <p>到这里就会引出一个新的问题：如何界定一个模型有解决某类问题的能力？ 从Markov chain的视角来看，问题本身就是这个链上的一个状态集合A（称之为问题状态集，之所以是个集合是因为同一个问题有很多表示形式，在链上的状态数必然不止一个），而我们期望的答案也是这个链上的一个状态集合B（称之为答案状态集）。只要在这个链上从A到B的转移概率不为0，那么我们就可以认为模型是具有解决这个问题的能力的。用公式表达就是：</p> \[P_{LLM}(B|A) &gt; 0 \Rightarrow {LLM有能力解决问题A}\] <p>所以说，如果模型在某种Prompt的提示下产生了期望的输出，那么我们就能认为模型本身是具有能力的，只不过被Prompt激发了出来。</p> <p>有趣的是，在计算机视觉领域，据我所知还没有类似Prompt这种可以激发单一模态的视觉模型能力的方法（大部分没有训练过的数据增强策略都对效果有负面影响）。</p> <p>其实这个视角同样可以套用到人的身上：<strong>如果一个人存在解决某个问题的可能性（解决问题的概率大于0），那我们就能认为这个人是有能力解决这个问题的</strong>。</p> <p>其次想要聊的是LM对于拥有更大信息量的数据的偏好性。</p> <p>这点其实也很好理解，同样长度的数据，如果LM在看过数据之后，对应的Markov chain中的转移概率没有发生太大的变化，那么这个数据训练与否对LM并没有太大的影响，反倒是一些能够改变Markov chain中转移概率的数据起到的作用更大。换句话说，在训练过程中，LM更倾向于受到具有更大信息量的数据的影响，因为这些数据可以帮助Markov chain建立状态之间的链接，修正状态之间的转移概率。</p> <p>套用到人身上，就是已知的信息看再多遍也很难有明显的提升，提升自己的能力靠的是寻求新的知识与挖掘看待旧知识的新视角。 而<strong>模型的预训练就像人类的学习一样，都是在初始链接的基础上，不断更新状态之间的转移概率，建立更强的状态间的连接</strong>。</p> <h4 id="新视角下的old-things">新视角下的Old things</h4> <p>这个部分我们会站在Markov chain的新视角来看待一些“旧事物”。</p> <h5 id="prompt-engineering">Prompt Engineering</h5> <p>在<a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Lilian Weng</a>介绍Prompt Enginerring的<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" rel="external nofollow noopener" target="_blank">blog</a><sup>[3]</sup>以及<a href="https://www.promptingguide.ai/" rel="external nofollow noopener" target="_blank">Prompting Guide网站</a>中介绍了很多prompt engineering的方法， 有一些方法对新人来说也许有一些反直觉或者tricky，比如把问题中的”Q”换成”Question”、一开始给LM设定一些特定的角色玩cosplay、 给几个实际样例(few-shot)，但是考虑到前文所述我们仅仅是要从问题状态A集找到一条到回答状态集B的一条转移路径，这些方法也就不难理解。</p> <p>以把原始问题中的”Q”换成”Question”这个trick为例，其所表达的就是下面一个朴素的公式：</p> \[P_{LLM}(Answer|Q \; type \; Prompt) &lt; P_{LLM}(Answer|Question \; type \; Prompt)\] <p>从Markov chain视角来看上面的公式：“question”状态转移到答案状态的概率，要比和“q”状态转移到答案的概率更高。</p> <p><strong>通过在输入端改变问题，进而改变问题状态集，并且最终提升转移到答案状态集的概率，这是在Markov chain视角下对于Prompt Engineering的新视角</strong>。</p> <h5 id="in-context-learning">In-Context Learning</h5> <p>In-Context Learning（下称ICL），简单来说，就是类似下面的一种场景：</p> <pre><code class="language-C++">评论： 这个电影太烂了。 态度：消极。
评论： 我好喜欢这个电影。 态度：
</code></pre> <p>模型则会根据输入对应产生输出。</p> <pre><code class="language-C++">评论： 这个电影太烂了。 态度：消极。
评论： 我好喜欢这个电影。 态度：积极。
</code></pre> <p>在华盛顿大学和meta研究ICL为什么能work的<a href="https://arxiv.org/abs/2202.12837" rel="external nofollow noopener" target="_blank">paper</a><sup>[6]</sup>里（或者参考斯坦福大学的<a href="http://ai.stanford.edu/blog/understanding-incontext/" rel="external nofollow noopener" target="_blank">blog</a><sup>[7]</sup>）， 研究人员探究了一下到底是输入、输出还是输入-输出的匹配更加重要（参考下图）。</p> <p><img src="/assets/blog/icl.png" alt="ICL" width="700"></p> <p>文章给出了一个非常有信息量的实验：<strong>输入-输出的匹配并没有想象中那么重要</strong>。也就是说，即使将原有标签随机修改，比如上面的示例修改成<code class="language-plaintext highlighter-rouge">评论： 这个电影太烂了。 态度：积极。</code>，模型仍然能够产生正确的输出。关键在于保持输入和输出本身的一致性。</p> <p>结合Markov chain来看待ICL：<strong>通过指定问题的输入和输出空间，使得LM在一个固定的子图上游走，使得模型更有可能产生正确的输出</strong>。最妙的是，根据实验来看，这个游走过程是不受之前的错误状态引导的。</p> <h5 id="cot">CoT</h5> <p>“Let’s think step by step.”<sup>[5]</sup>的魔法Prompt也被称为Zero-shot CoT（Chain of Thought），在使用了这样的prompt之后，模型更容易沿着分解问题的思路解决问题，从而在一些逻辑推理类的任务上产生分步输出，进而获取更接近真实答案的输出。</p> <p>在Markov Chain中，<strong>“Let’s think step by step” 和问题的中间步骤关联，而中间步骤状态相比没有任何输出的状态转移到答案的概率更高。</strong>这样来看，想出这个prompt也是很需要insight的。</p> <h4 id="random-but-not-random">Random, but not random</h4> <p>这个其实是我观察到的一个很有趣的现象，很多时候LLM是能够理解随机的，但是行为上却绝对做不到最真实的随机。其实从Markov chain的视角来看，这个事情是很容易理解的， 但是可能你去问一些ChatGPT的用户，他们或许也并不能回答这个问题：<strong><span style="color:red">如果要求ChatGPT完成如下的任务：“从A，B，C，D中随机选择一个”，那么ChatGPT这样的LM能否从做到统计意义上的随机？</span></strong></p> <p>答案很显然：<strong><span style="color:red">肯定不能，而且LM几乎确定不能原生解决这样的问题</span></strong>。要证明这个问题也很简单，以这个“ABCD”的例子 来说明，仅仅考虑当前Markov chain的状态S，考虑后续输出为“A”，“B”，“C”，“D”的四种状态A、B、C、D，要做到统计意义上的随机，Markov chain就一定需要满足下面的公式（不考虑temperature这些因素）：</p> \[P(A|S) = P(B|S) = P(C|S) = P(D|S) = 25\%\] <p>注意公式里面的ABCD只是一个状态的合集，也就是像“A”和“A.“都是A这个集合中的一个元素，所以说LM几乎确定不能解决这个问题。 但是如果引入插件的思想，由LM做控制器来判断需要执行<code class="language-plaintext highlighter-rouge">random.choice(["A", "B", "C", "D"])</code>函数，这个问题就非常容易解决了。</p> <h4 id="citation">Citation</h4> <p>如果觉得有帮助，欢迎引用这篇blog：</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article<span class="o">{</span>wang2023LLM,
  title   <span class="o">=</span> <span class="s2">"LLMs as Markov Chain"</span>,
  author  <span class="o">=</span> <span class="s2">"Wang, Feng"</span>,
  journal <span class="o">=</span> <span class="s2">"fatescript.github.io"</span>,
  year    <span class="o">=</span> <span class="s2">"2023"</span>,
  month   <span class="o">=</span> <span class="s2">"Jun"</span>,
  url     <span class="o">=</span> <span class="s2">"https://fatescript.github.io/blog/2023/LLM-markov-chain/"</span>
<span class="o">}</span>
</code></pre></div></div> <h4 id="reference">Reference</h4> <p><strong>[1]</strong> <a href="https://twitter.com/karpathy/status/1645115622517542913" rel="external nofollow noopener" target="_blank">Karpathy的twiiter</a><br> <strong>[2]</strong> <a href="https://t.co/8jdceMLpqy" rel="external nofollow noopener" target="_blank">介绍LLM as Markov Chain的Colab</a><br> <strong>[3]</strong> <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" rel="external nofollow noopener" target="_blank">Lilian Weng介绍Prompt Engineering的blog</a><br> <strong>[4]</strong> <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> <br> <strong>[5]</strong> <a href="https://arxiv.org/abs/2205.11916" rel="external nofollow noopener" target="_blank">Large Language Models are Zero-Shot Reasoners</a><br> <strong>[6]</strong> <a href="https://arxiv.org/abs/2202.12837" rel="external nofollow noopener" target="_blank">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a><br> <strong>[7]</strong> <a href="http://ai.stanford.edu/blog/understanding-incontext/" rel="external nofollow noopener" target="_blank">How does in-context learning work?</a></p> </div> </div> </article> <div class="pagenav"> <a class="previous-post" href="/blog/2023/work-and-think/"> <span>⬅️</span><br> <span class="smaller-title"><strong>我在贵司这三年</strong></span> </a> <a class="next-post" href="/blog/2022/copybara/"> <span>➡️</span><br> <span><strong>copybara：关于我只是想在仓库间做代码搬运这件事</strong></span> </a> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"FateScript/FateScript.github.io","data-repo-id":"R_kgDOH71xZQ","data-category":"Announcements","data-category-id":"DIC_kwDOH71xZc4CZgZI","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"zh-CN",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Feng Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-EH97TKHJR3"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EH97TKHJR3");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>